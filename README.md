# ONE BILLION CHALLENGE ‚Äì PYTHON EDITION

## ABOUT THE PROJECT

Uma jornada pr√°tica e realista de engenharia de dados para processar 1 bilh√£o de registros, extraindo estat√≠sticas agregadas de temperatura com performance, escalabilidade em Python, utilizando o projeto One Billion Row Challenge, desenvolvido como um exerc√≠cio avan√ßado de engenharia de dados aplicada, com o objetivo de demonstrar como processar com efici√™ncia um arquivo massivo de 1 bilh√£o de linhas (~14GB) usando Python, cujo foco est√° em realizar opera√ß√µes computacionalmente intensas como agrega√ß√µes (m√≠nimo, m√©dia e m√°ximo) e ordena√ß√£o com uso criterioso de recursos computacionais, de forma escal√°vel.

Este projeto √© particularmente √∫til como estudo de caso para engenheiros de dados, cientistas de dados e desenvolvedores que desejam aprofundar seus conhecimentos em processamento de arquivos massivos, estrat√©gias de chunking, desempenho de bibliotecas Python e uso de engines anal√≠ticas modernas como o DuckDB e embora o One Billion Row Challenge n√£o seja um projeto t√©cnico, ele simula situa√ß√µes reais de neg√≥cio enfrentadas por empresas que lidam com grandes volumes de dados transacionais, sensoriais ou operacionais.

## INSPIRATION

O desafio foi inspirado no projeto original [1BRC](https://github.com/gunnarmorling/1brc), proposto por Gunnar Morling, em Java, posteriormente, a iniciativa foi adaptada para Python por Luciano Vasconcelos, no reposit√≥rio [One-Billion-Row-Challenge-Python](https://github.com/lvgalvao/One-Billion-Row-Challenge-Python), como um workshop, dentro do contexto educacional da Jornada de Dados, em 2024.

---

## BUSINESS PROBLEM

A seguir, destacam-se os principais problemas que esse case ajuda a resolver:

‚û°Ô∏è 1. Processamento de Grandes Volumes de Dados em Arquivos Brutos, quando empresas frequentemente recebem dados em formatos como .csv, .json ou .parquet contendo milh√µes ou bilh√µes de linhas, especialmente em setores como varejo, energia, climatologia, IoT e telecom, demonstrando como ler, limpar e agregar dados diretamente de arquivos massivos, sem a necessidade imediata de carregar tudo na mem√≥ria ou depender de clusters caros.

‚û°Ô∏è 2. C√°lculo Eficiente de Estat√≠sticas Agregadas, por meio da an√°lise de dados operacionais exige c√°lculos como m√©dia, m√°ximo e m√≠nimo, que parecem simples, mas se tornam desafiadores com grande volume e m√∫ltiplas chaves, o case mostra como aplicar estrat√©gias otimizadas de agrega√ß√£o, inclusive via DuckDB ou Pandas com chunking, simulando o c√°lculo de indicadores operacionais em escala.

‚û°Ô∏è 3. Desempenho e Otimiza√ß√£o de Recursos Computacionais, quando projetos de dados nem sempre rodam em ambientes robustos, muitos times enfrentam limita√ß√µes de RAM, CPU e I/O, especialmente em pipelines locais, servidores intermedi√°rios ou jobs agendados, explorando estrat√©gias de baixo consumo de mem√≥ria, chunking e uso de engines colunares (como DuckDB) que permitem otimizar desempenho mesmo em m√°quinas comuns.

‚û°Ô∏è 4. Valida√ß√£o de Arquiteturas Anal√≠ticas para Batch Processing, no processo de valida√ß√£o, por exemplo, se uma arquitetura (ex: processamento local + exporta√ß√£o .parquet) atende aos SLAs de tempo e custo antes de mover dados para a nuvem, fornecendo um sandbox completo e replic√°vel, permitindo testar pipelines de processamento, benchmarkar formatos de arquivo e comparar abordagens de leitura e agrega√ß√£o.

‚û°Ô∏è 5. Treinamento e Capacita√ß√£o T√©cnica de Times de Dados, para formar times com maturidade em engenharia de dados exige cases pr√°ticos e desafiadores, que v√£o al√©m de notebooks pequenos ou datasets de toy, demonstrando ser um estudo de caso avan√ßado que pode ser usado para treinar engenheiros, analistas e cientistas de dados, com foco em performance, arquitetura de dados e boas pr√°ticas de codifica√ß√£o.

‚û°Ô∏è 6. Exporta√ß√£o de Dados para Consumo em BI e Visualiza√ß√µes, etapa comum a necessidade de transformar arquivos brutos em formatos eficientes para dashboards (como .csv limpo ou .parquet otimizado), gerando outputs padronizados e ordenados para ingest√£o por ferramentas como Power BI, Metabase, Superset ou solu√ß√µes em nuvem, com foco em consumo r√°pido e leve.

---

## PROPOSED CHALLENGE

Desenvolver solu√ß√µes em Python para:

üîπLer o arquivo de entrada com 1 bilh√£o de linhas

üîπCalcular, para cada esta√ß√£o:
- Temperatura m√≠nima
- Temperatura m√°xima
- Temperatura m√©dia (com 2 casas decimais)

üîπOrdenar os resultados por nome da esta√ß√£o

üîπExportar os resultados para os formatos .csv e .parquet

üîπComparar diferentes abordagens de performance, mem√≥ria e escalabilidade

### DATA STRUCTURE

O arquivo de entrada cont√©m medi√ß√µes de temperatura de diferentes esta√ß√µes meteorol√≥gicas, com o seguinte formato por linha:

---

### GENERAL OPERATION

1. Valida√ß√£o dos Argumentos, pois o script recebe como argumento a quantidade de linhas a serem geradas.

2. Coleta de Nomes de Esta√ß√µes, l√™ um arquivo chamado `model.csv` com nomes de esta√ß√µes meteorol√≥gicas.

3. Remove duplicatas e ignora linhas comentadas com `#`, ainda devolve uma estimativa de tamanho do arquivo

4. Calcula o tamanho estimado do arquivo final com base na m√©dia de caracteres dos nomes das esta√ß√µes e nas temperaturas geradas, com a gera√ß√£o de dados sint√©ticos

5. Cria medi√ß√µes com temperaturas aleat√≥rias entre -99.9¬∞C e 99.9¬∞C e gera o arquivo `data/weather_stations.csv`.

6. Utiliza processamento em batches de 10.000 registros para melhor desempenho de escrita e apresenta uma barra de progresso ao longo da execu√ß√£o.

7. Medi√ß√µes de Performance sendo que ao final, mostra o tempo total de execu√ß√£o e o tamanho real do arquivo gerado.

---

### OUTPUT FILE

Arquivo gerado `data/weather_stations.csv` em 6 min e 5 seg, com 14.8 GiB, somando 1 bilh√£o de linhas.

```text
<nome_da_esta√ß√£o>;<temperatura>
```
- nome_da_esta√ß√£o: string
- temperatura: float com precis√£o de duas casas decimais

Exemplo:

```text
Stockholm;-5.32
S√£o Paulo;25.85
Cape Town;19.01
```
---

### INTERESTING TECHNICAL POINTS

- Evita uso de `round()` para performance, usando `f"{x:.1f}"` para limitar casas decimais
- Usa `random.choices()` para gerar esta√ß√µes com distribui√ß√£o uniforme entre nomes v√°lidos
- Escreve dados em lote para evitar overhead de I/O, linha a linha
- Estima o uso de disco antes da gera√ß√£o, com fun√ß√£o personalizada para convers√£o de bytes
- Fornece mensagens amig√°veis de erro e ajuda ao usu√°rio

---

### IMPLEMENTED APPROACHES

üîπ Leitura Linha a Linha (Streaming Puro - Python Nativo)
- Uso de leitura sequencial com open() + readline()
- Agrega√ß√µes realizadas em tempo real com dicion√°rios
- Estrat√©gia eficiente em consumo de mem√≥ria (low RAM footprint)
- Ideal para ambientes com recursos limitados

üîπ Chunking Manual (Python Nativo com Divis√£o em Blocos)
- T√©cnica de leitura em blocos (ex: 1 milh√£o de linhas por vez)
- Reduz picos de mem√≥ria e melhora o controle do processamento
- √ötil para ajustes finos de performance e paraleliza√ß√£o

üîπ Pandas (DataFrame Completo)
- Abordagem de leitura em lote √∫nico com pd.read_csv()
- Permite uso de fun√ß√µes vetorizadas e agrega√ß√µes r√°pidas
- Limita√ß√µes para m√°quinas com <16GB RAM

üîπ Pandas com Chunking (pd.read_csv(..., chunksize=N))
- Divide o dataset em mini-DataFrames processados iterativamente
- Une a performance do Pandas com escalabilidade de pipelines
- √ötil para ambientes em nuvem com controle de mem√≥ria

üîπ DuckDB (Processamento Colunar com SQL Embutido)
- Engine anal√≠tica colunar embutida (sem servidor)
- Altamente otimizada para workloads de leitura pesada
- Permite uso de SQL para agrega√ß√µes diretas no arquivo .csv
- Suporte nativo a .parquet, integra√ß√£o direta com Pandas, Apache Arrow e Python

---

## HOW TO RUN

### REQUIREMENTS

1. Git e Github: Utilizado para versionamento do c√≥digo e para reposit√≥rio remoto do projeto.
Voc√™ deve ter o Git instalado em sua m√°quina e tamb√©m deve ter uma conta no GitHub.
[Instru√ß√µes de instala√ß√£o do Git aqui](https://git-scm.com/doc).
[Instru√ß√µes de instala√ß√£o do Github aqui](https://docs.github.com/pt).

2. Pyenv: √â usado para gerenciar vers√µes do Python em ambientes virtuais, fundamental para isolar a aplica√ß√£o e evitar problemas de conflitos entre vers√µes de bibliotecas e do pr√≥prio Python.
[Instru√ß√µes de instala√ß√£o do Pyenv aqui](https://github.com/pyenv/pyenv#installation).
Neste projeto, vamos utilizar o Python 3.11.4

3. Poetry: Este projeto utiliza Poetry para gerenciamento de depend√™ncias.
[Instru√ß√µes de instala√ß√£o do Poetry aqui](https://python-poetry.org/docs/#installation).

---

### INSTALA√á√ÉO E CONFIGURA√á√ÉO

A - Execute o comando, passando os argumentos da quantidade de linhas que quer gerar:

```python
python create_measurements.py 1_000_000_000
```
---

B - Confirmar a quantidade de linhas e o formato do arquivo gerado:
```python
wc -l ../data/weather_stations.csv
head -n 5 ../data/weather_stations.csv
```

C - Entre no diret√≥rio `/data/`e execute o comando, de acordo com a ferramenta e amodelagem de dados desejada:

1) Python - processamento BRUTO com `defaultdict`, utilizando Python vanilla!
```python
python etl_python.py
```

2) Python com chuncking
```python
python etl_python_chuncking.py
```

3) Pandas
```python
python etl_pandas.py
```

4) Pandas com chuncking
```python
python etl_pandas_chuncking.py
```

5) Instale a biblioteca duckDB, utilizando o Poetry, com o comando:
```python
poetry add duckdb
```

6) duckDB
```python
python etl_duckDB.py
```

---

## OUTPUT EXAMPLES

Todos os resultados finais s√£o exportados nos formatos .csv e .parquet

Isso permite an√°lises posteriores em ferramentas como Power BI, Metabase, Apache Superset ou puro Python, inclusive, o arquivo de sa√≠da ser√° ordenado alfabeticamente por nome da esta√ß√£o:

```python
| Esta√ß√£o | Min | M√©dia | Max|
| ---------- | ------ | ----- | ----- |
| Aabenraa| -99.80 | 3.4| 99.80 |
| Bariloche  | -57.40 | 8.2| 87.30 |
| Copenhagen | -45.50 | 11.9  | 94.10 |
```

---

## TECHNOLOGIES USED

### PROJECT SUPPORT

üîπ Poetry para gerenciamento de depend√™ncias

üîπ Pyenv para isolamento de ambientes

üîπ Pre-commit hooks:
- trailing-whitespace
- end-of-file-fixer
- check-yaml
- check-added-large-files
- check-json
- check-merge-conflict
- check-case-conflict

üîπ Pip-audit

üîπ Black

üîπ Ruff

---

### PROJECT DEVELOPMENT

üîπPython 3.11+

üîπPandas

üîπPyarrow

üîπPolars

üîπDuckDB

---

## BENCHMARKING AND PERFORMANCE

### PYTHON
üî¥ Python vanilla, sem utiliza√ß√£o de ulimit ou cgroups.
A ETL quebrou por 6 vezes, consumindo os 16 GiB (15.3) de mem√≥ria RAM do servidor e mais 4 de Swp

üü® Python Vanilla com melhorias de performance:
A ETL rodou satisfatoriamente, demorando 726.20 segundos (pouco mais de 12 minutos) e consumindo apenas 1.5 GiB de mem√≥ria RAM, no momento de pico de utiliza√ß√£o do sistema.

üü® Python com a utiliza√ß√£o de t√©cnica de chunking
A ETL rodou sofrida, n√£o aguentou com chuncking de 100 milh√µes de linhas, quebrando duas vezes, rodando com chuncking de 50 milh√µes de linhas em 20 etapas, demorando 1436.41 segundos (quase 24 minutos) e consumindo 12.2 GiB de mem√≥ria RAM, no momento de pico de utiliza√ß√£o do sistema.

---

### PYTHON + PYARROW
üü® Python com a utiliza√ß√£o da biblioteca pyarrow apenas para gravar o parquet.
A ETL rodou satisfatoriamente, demorando 711.31 segundos (quase 12 minutos) e consumindo apenas 1.2 GiB de mem√≥ria RAM, no momento de pico de utiliza√ß√£o do sistema.

---

### PYTHON + PANDAS
üî¥ Python + Pandas na leitura e no processamento
A ETL quebrou por 3 vezes, consumindo os 16 GiB (15.3) de mem√≥ria RAM do servidor e mais 4 de Swp

üü® Python + Pandas na leitura e no processamento + utiliza√ß√£o de t√©cnica de chunking
A ETL rodou satisfatoriamente, rodou com chuncking de 100 milh√µes de linhas, demorando 348.58 segundos (quase 6 minutos) e consumindo 10 GiB de mem√≥ria RAM, no momento de pico de utiliza√ß√£o do sistema.

---

### PYTHON + POLARS
üî¥ Python + Polars na leitura e no processamento
A ETL quebrou por 3 vezes, em 5 segundos, consumindo os 16 GiB (15.3) de mem√≥ria RAM do servidor e mais 4 de Swp

üî¥ Python + Polars na leitura e no processamento + utiliza√ß√£o de t√©cnica de paralelismo
A ETL quebrou por 3 vezes, em 5 segundos, consumindo os 16 GiB (15.3) de mem√≥ria RAM do servidor e mais 4 de Swp

---

### duckDB
üü¢ Utiliza√ß√£o do banco de dados duckDB ü•á üèÜ
A ETL rodou lisa, demorando 12.38 segundos e consumindo apenas 1.76 GiB de mem√≥ria RAM, no momento de pico de utiliza√ß√£o do sistema.


## CONCLUSION

O benchmark conduzido com 1 bilh√£o de registros sint√©ticos de esta√ß√µes meteorol√≥gicas revela insights importantes sobre tempo de execu√ß√£o, uso de mem√≥ria, tamanho dos arquivos e escalabilidade entre diferentes estrat√©gias de processamento: Python puro, Pandas, abordagens com chunking, Polars e DuckDB.

### 1. ‚è±Ô∏è Tempo de Execu√ß√£o Total

- DuckDB manteve seu desempenho superior, concluindo a ETL em apenas 12.38 segundos, mesmo com 1 bilh√£o de linhas.
- Pandas com chunking foi a abordagem tradicional mais eficiente, concluindo em 348.58 segundos (~6 minutos).
- Python puro com melhorias levou 726.20 segundos (~12 minutos), com performance est√°vel.
- Python com chunking precisou de m√∫ltiplas etapas (20 chunks de 50 milh√µes), totalizando 1436.41 segundos (~24 minutos).
- As abordagens com Polars e Pandas sem chunking falharam devido ao estouro de mem√≥ria, n√£o completando a execu√ß√£o.

![total_time](image.png)

DuckDB novamente se destaca por sua efici√™ncia vetorizada e engine SQL em mem√≥ria. Chunking com Pandas ou Python √© mais lento, mas confi√°vel quando h√° limita√ß√£o de RAM.

---

### 2. üíæ Pico de Uso de Mem√≥ria RAM

- Python + PyArrow (escrevendo apenas Parquet com PyArrow) foi o mais econ√¥mico, com pico de 1.2 GiB.
- DuckDB tamb√©m se manteve enxuto, consumindo apenas 1.76 GiB.
- Python + melhorias estabilizou em 1.5 GiB.
- Pandas com chunking usou 10 GiB, demonstrando bom controle.
- Python com chunking chegou a 12.2 GiB.
- Pandas, Polars e outras abordagens sem chunking estouraram os 16 GiB de RAM + 4 GiB de swap, travando a execu√ß√£o.

![total_memory](image-1.png)

DuckDB e PyArrow mant√™m uso controlado de mem√≥ria. Abordagens com chunking consomem mais, mas s√£o seguras. Estrat√©gias sem chunking falham com volumes bilion√°rios.

---

### 3. üì¶ Tamanho dos Arquivos (MiB)

Todos os arquivos CSV t√™m tamanho semelhante (~252 KB). DuckDB gerou o menor .csv e tamb√©m o .parquet mais compacto, evidenciando compress√£o eficiente e escrita otimizada.

![total_file_size](image-2.png)

---

### Considera√ß√µes de Arquitetura e Escalabilidade

- DuckDB permanece como a op√ß√£o mais r√°pida, leve e escal√°vel para an√°lise local, com excelente performance mesmo com 1 bilh√£o de registros.
- Pandas + chunking se mostra um bom compromisso para ambientes com restri√ß√£o de mem√≥ria, sem comprometer robustez.
- Python puro com chunking √© funcional, mas requer ajustes e monitoramento rigoroso de recursos.
- Polars ainda n√£o sustentou o volume testado ‚Äî falhou em todas as tentativas mesmo com paralelismo ativado.

### Recomenda√ß√µes Finais

Para pipelines de grande volume com baixa complexidade de transforma√ß√£o e foco em performance:

- ‚úÖ DuckDB continua imbat√≠vel: r√°pido, econ√¥mico e com boa compress√£o.
- üü° Pandas com chunking √© seguro, compat√≠vel e f√°cil de manter.
- üü° Python com chunking √© defens√°vel, mas exige mais trabalho manual.
- üî¥ Abordagens sem chunking n√£o s√£o recomendadas acima de 1 bilh√£o de linhas.
- ‚û°Ô∏è DuckDB √© a escolha mais enxuta, tanto em CSV quanto em Parquet.
- ‚û°Ô∏è Parquet √© amplamente superior ao CSV em termos de efici√™ncia de armazenamento e preparo para an√°lise.

Nem todo projeto de dados exige alta performance ou infraestrutura distribu√≠da, mas saber escolher a abordagem certa para o contexto certo √© o que separa scripts r√°pidos de pipelines confi√°veis e sustent√°veis.

Seus testes mostraram que cada tecnologia se comporta de forma distinta frente a tr√™s fatores cr√≠ticos: volume de dados, disponibilidade de mem√≥ria RAM e necessidade de escalabilidade.

Abaixo, a an√°lise de cada abordagem sob esse prisma:

#### DuckDB: r√°pido, leve e pronto para escalar localmente

O DuckDB demonstrou ser o motor mais equilibrado para an√°lises locais e pipelines de pequeno e m√©dio porte.

Por que funciona t√£o bem?

- Ele processa os dados direto do disco, sem precisar carreg√°-los inteiramente na mem√≥ria.
- Seu modelo de execu√ß√£o √© colunar e vetorizado, o que significa que cada opera√ß√£o trabalha por blocos otimizados, aproveitando o cache do processador.
- Funciona bem mesmo com apenas um n√∫cleo (monothread), o que o torna ideal para ambientes simples, como notebooks ou servidores de uso geral.

‚úÖ Ideal para prot√≥tipos r√°pidos e ETLs locais com performance real, ambientes com pouca RAM ou CPU

############################### ‚õî CUIDADO ###############################

O DuckDB √© uma ferramenta poderosa, mas como toda tecnologia, tem um conjunto de casos para os quais √© ideal e outros onde n√£o √© a melhor escolha, conforme segue:

## ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è DuckDB √© excelente para prototipagem, an√°lise local, cargas moderadas e dados tabulares em formato Parquet, CSV, Arrow, mas por que DuckDB n√£o √© geralmente indicado para produ√ß√£o?

###  Limita√ß√µes do DuckDB em produ√ß√£o, explicadas por contexto

#### 1. Arquitetura embutida, n√£o cliente-servidor

- DuckDB roda embutido no processo da aplica√ß√£o (embedded database), isso quer dizer que n√£o h√° um servidor separado para lidar com concorr√™ncia, autentica√ß√£o, escalabilidade, etc., em produ√ß√£o, espera-se que o banco aceite m√∫ltiplas conex√µes, distribua carga e possa ser escalado horizontalmente.

üëâ Consequ√™ncia: DuckDB √© monousu√°rio por design, se m√∫ltiplas aplica√ß√µes ou usu√°rios tentarem acessar o mesmo banco simultaneamente, voc√™ corre risco de corrup√ß√£o ou race conditions.

---

#### 2. N√£o suporta m√∫ltiplas sess√µes concorrentes de escrita

- Em bancos como PostgreSQL, m√∫ltiplos processos podem ler e escrever simultaneamente, com controle de transa√ß√µes.
- O DuckDB s√≥ permite uma escrita por vez e ainda bloqueia arquivos `.duckdb` durante a opera√ß√£o.

üëâ Consequ√™ncia: Impratic√°vel em ambientes multiusu√°rio, com alta taxa de grava√ß√£o ou uso concorrente, como APIs, microsservi√ßos e sistemas OLTP.

---

### 3. N√£o √© tolerante a falhas por padr√£o

- Bancos de produ√ß√£o geralmente contam com replica√ß√£o, backups autom√°ticos, failover, logs de transa√ß√£o para recovery e DuckDB n√£o implementa esses recursos nativamente.

üëâ Consequ√™ncia: Se seu processo for interrompido abruptamente (por crash, falha de disco ou interrup√ß√£o de energia), voc√™ pode perder o banco ou corromper o arquivo.

---

### 4. N√£o escala horizontalmente

- DuckDB n√£o possui arquitetura distribu√≠da, ele n√£o foi feito para escalar em m√∫ltiplas m√°quinas nem processar grandes volumes em cluster (como Spark, Dask, BigQuery, etc).

üëâ Consequ√™ncia: Para dados de alta volumetria (> bilh√µes de linhas), ou para times que precisam escalar a leitura e escrita em paralelo, o DuckDB n√£o acompanha.

---

### 5. Foco principal √© an√°lise local ‚Äî OLAP, n√£o OLTP

- DuckDB √© orientado a consultas anal√≠ticas complexas (OLAP), n√£o a sistemas transacionais (OLTP), ele brilha ao fazer `SELECT station, AVG(temp)` em 100 milh√µes de linha, mas n√£o serve bem para registrar pedidos de e-commerce ou gerenciar usu√°rios de um app em tempo real.

üëâ Consequ√™ncia: N√£o use DuckDB para aplica√ß√µes com alta taxa de inser√ß√£o, atualiza√ß√£o ou leitura em tempo real.

---

### Use DuckDB com confian√ßa para:

![alt text](image-5.png)

Quando se trata de uso do DuckDB como fonte de dados para dashboards ‚Äî como Power BI, Metabase, Superset ou at√© Streamlit ‚Äî a an√°lise muda bastante, e a resposta √© "depende do uso, mas com ressalvas importantes".

---

## ‚úÖ DuckDB √© uma excelente FONTE para dashboards... se usado da maneira certa

### üü¢ Vantagens

1. Leitura muito r√°pida:
- Consultas anal√≠ticas (`SELECT`, `GROUP BY`, `JOIN`) s√£o extremamente otimizadas em DuckDB, principalmente em formatos como Parquet e CSV.
- Ideal para pain√©is que consultam dados prontos, agregados.
2. Compat√≠vel com ODBC/ODBC-like connectors:
- DuckDB oferece conectores (em evolu√ß√£o) para se integrar com BI tools, especialmente via drivers ODBC.
- J√° existem m√©todos para conectar o Power BI via ODBC e o Metabase via JDBC/ODBC (com algum esfor√ßo).
3. Formato leve e port√°til:
- O `.duckdb` √© um √∫nico arquivo. Voc√™ pode gerar e compartilhar com o dashboard, sem precisar de um servidor de banco.
4. Integra√ß√£o com Parquet e CSV:
- DuckDB pode ser usado para consultar diretamente arquivos Parquet/CSV como se fossem tabelas ‚Äî √∫til quando seu dashboard √© alimentado por arquivos externos.

---

## ‚ö†Ô∏è Limita√ß√µes e cuidados

### 1. N√£o √© um servidor de banco ‚Äî logo, sem pooling, DuckDB n√£o escuta conex√µes TCP como PostgreSQL/MySQL, cada dashboard teria que abrir a base localmente, o que n√£o escala para m√∫ltiplos usu√°rios. Uma solu√ß√£o parcial seria o uso do DuckDB em cen√°rios de *data refresh batch*, ou seja, voc√™ gera um CSV ou Parquet com DuckDB e usa esse arquivo como fonte no Power BI ou Metabase, que apontam para ele.

### 2. Sem controle de concorr√™ncia

- Se dois dashboards tentarem acessar simultaneamente o mesmo arquivo `.duckdb`, pode haver corrup√ß√£o ou travamento, a recomenda√ß√£o √© que evite `.duckdb` como fonte compartilhada de leitura concorrente em dashboards, prefira extrair e gerar `.parquet` ou `.csv` de leitura r√°pida.


## ‚úÖ Recomenda√ß√µes pr√°ticas
![duckDB](image-4.png)

---

## üí° Conclus√£o

DuckDB √© extremamente eficaz para gerar datasets anal√≠ticos e alimentadores de dashboard, mas n√£o √© ideal como fonte de dados din√¢mica e concorrente.

‚úÖ Use DuckDB para processar e entregar dados prontos para visualiza√ß√£o.

‚ö†Ô∏è Evite us√°-lo como backend direto em dashboards multiusu√°rio em produ√ß√£o.

üí° Melhor estrat√©gia: DuckDB ‚Üí Parquet ‚Üí Dashboard.


---

## MAIN TECHNICAL FEATURES

‚úÖ Modular function design (read, calculate, format, log)

‚úÖ Log file with timestamps and emojis for readability

‚úÖ Automatic folder creation for logs

‚úÖ Fast performance with native Python (no Pandas or NumPy required)

‚úÖ Friendly CLI usage, expandable to larger systems

---

Project carried out with the support of Artificial Intelligence (ChatGPT)

For future improvements: extraction of real data with cleaning and transformation, followed by loading into a Data Warehouse, possibly in a cloud provider, also, an ETL orchestrated with Apache Airflow and best CI/CD practices


## QUESTIONS, SUGGESTIONS OR FEEDBACK

üöÄ Andr√© Matiello C. Caramanti - [matiello.andre@hotmail.com](mailto:matiello.andre@hotmail.com)

---

## LICENSE

[MIT License](https://andrematiello.notion.site/mit-license)
