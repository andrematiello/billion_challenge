# ONE BILLION CHALLENGE ‚Äì PYTHON EDITION

## ABOUT THE PROJECT

Uma jornada pr√°tica e realista de engenharia de dados para processar 1 bilh√£o de registros, extraindo estat√≠sticas agregadas de temperatura com performance, escalabilidade em Python, utilizando o projeto **One Billion Row Challenge**, desenvolvido como um exerc√≠cio avan√ßado de engenharia de dados aplicada, com o objetivo de demonstrar como processar com efici√™ncia um arquivo massivo de 1 bilh√£o de linhas (~14GB) usando Python, cujo foco est√° em realizar opera√ß√µes computacionalmente intensas como agrega√ß√µes (m√≠nimo, m√©dia e m√°ximo) e ordena√ß√£o com uso criterioso de recursos computacionais, de forma escal√°vel.

Este projeto √© particularmente √∫til como estudo de caso para engenheiros de dados, cientistas de dados e desenvolvedores que desejam aprofundar seus conhecimentos em processamento de arquivos massivos, estrat√©gias de chunking, desempenho de bibliotecas Python e uso de engines anal√≠ticas modernas como o DuckDB e embora o One Billion Row Challenge n√£o seja um projeto t√©cnico, ele simula situa√ß√µes reais de neg√≥cio enfrentadas por empresas que lidam com grandes volumes de dados transacionais, sensoriais ou operacionais.

## INSPIRATION

O desafio foi inspirado no projeto original [1BRC](https://github.com/gunnarmorling/1brc), proposto por Gunnar Morling em Java, com o esp√≠rito:

> ‚ÄúExplore at√© onde as linguagens modernas podem ir ao processar um bilh√£o de linhas, use todos os (v)n√∫cleos, SIMD, otimiza√ß√µes de GC... e crie a implementa√ß√£o mais r√°pida para resolver esse problema!‚Äù

Posteriormente, a iniciativa foi adaptada para Python por Luciano Vasconcelos, no reposit√≥rio [One-Billion-Row-Challenge-Python](https://github.com/lvgalvao/One-Billion-Row-Challenge-Python), como um workshop, dentro do contexto educacional da Jornada de Dados, em 2024.


---

## BUSINESS PROBLEM

A seguir, destacam-se os principais problemas que esse case ajuda a resolver:

‚û°Ô∏è 1. Processamento de Grandes Volumes de Dados em Arquivos Brutos, quando empresas frequentemente recebem dados em formatos como .csv, .json ou .parquet contendo milh√µes ou bilh√µes de linhas, especialmente em setores como varejo, energia, climatologia, IoT e telecom, demonstrando como ler, limpar e agregar dados diretamente de arquivos massivos, sem a necessidade imediata de carregar tudo na mem√≥ria ou depender de clusters caros.

‚û°Ô∏è 2. C√°lculo Eficiente de Estat√≠sticas Agregadas, por meio da an√°lise de dados operacionais exige c√°lculos como m√©dia, m√°ximo e m√≠nimo, que parecem simples, mas se tornam desafiadores com grande volume e m√∫ltiplas chaves, o case mostra como aplicar estrat√©gias otimizadas de agrega√ß√£o, inclusive via DuckDB ou Pandas com chunking, simulando o c√°lculo de indicadores operacionais em escala.

‚û°Ô∏è 3. Desempenho e Otimiza√ß√£o de Recursos Computacionais, quando projetos de dados nem sempre rodam em ambientes robustos, muitos times enfrentam limita√ß√µes de RAM, CPU e I/O, especialmente em pipelines locais, servidores intermedi√°rios ou jobs agendados, explorando estrat√©gias de baixo consumo de mem√≥ria, chunking e uso de engines colunares (como DuckDB) que permitem otimizar desempenho mesmo em m√°quinas comuns.

‚û°Ô∏è 4. Valida√ß√£o de Arquiteturas Anal√≠ticas para Batch Processing, no processo de valida√ß√£o, por exemplo, se uma arquitetura (ex: processamento local + exporta√ß√£o .parquet) atende aos SLAs de tempo e custo antes de mover dados para a nuvem, fornecendo um sandbox completo e replic√°vel, permitindo testar pipelines de processamento, benchmarkar formatos de arquivo e comparar abordagens de leitura e agrega√ß√£o.

‚û°Ô∏è 5. Treinamento e Capacita√ß√£o T√©cnica de Times de Dados, para formar times com maturidade em engenharia de dados exige cases pr√°ticos e desafiadores, que v√£o al√©m de notebooks pequenos ou datasets de toy, demonstrando ser um estudo de caso avan√ßado que pode ser usado para treinar engenheiros, analistas e cientistas de dados, com foco em performance, arquitetura de dados e boas pr√°ticas de codifica√ß√£o.

‚û°Ô∏è 6. Exporta√ß√£o de Dados para Consumo em BI e Visualiza√ß√µes, etapa comum a necessidade de transformar arquivos brutos em formatos eficientes para dashboards (como .csv limpo ou .parquet otimizado), gerando outputs padronizados e ordenados para ingest√£o por ferramentas como Power BI, Metabase, Superset ou solu√ß√µes em nuvem, com foco em consumo r√°pido e leve.

---

## PROPOSED CHALLENGE

Desenvolver solu√ß√µes em Python para:

üîπLer o arquivo de entrada com 1 bilh√£o de linhas

üîπCalcular, para cada esta√ß√£o:
- Temperatura m√≠nima
- Temperatura m√°xima
- Temperatura m√©dia (com 2 casas decimais)

üîπOrdenar os resultados por nome da esta√ß√£o

üîπExportar os resultados para os formatos .csv e .parquet

üîπComparar diferentes abordagens de performance, mem√≥ria e escalabilidade

### DATA STRUCTURE

O arquivo de entrada cont√©m medi√ß√µes de temperatura de diferentes esta√ß√µes meteorol√≥gicas, com o seguinte formato por linha:

### GENERAL OPERATION

1. Valida√ß√£o dos Argumentos, pois o script recebe como argumento a quantidade de linhas a serem geradas.

2. Coleta de Nomes de Esta√ß√µes, l√™ um arquivo chamado `model.csv` com nomes de esta√ß√µes meteorol√≥gicas.

3. Remove duplicatas e ignora linhas comentadas com `#`, ainda devolve uma estimativa de tamanho do arquivo

4. Calcula o tamanho estimado do arquivo final com base na m√©dia de caracteres dos nomes das esta√ß√µes e nas temperaturas geradas, com a gera√ß√£o de dados sint√©ticos

5. Cria medi√ß√µes com temperaturas aleat√≥rias entre -99.9¬∞C e 99.9¬∞C e gera o arquivo `data/weather_stations.csv`.

6. Utiliza processamento em batches de 10.000 registros para melhor desempenho de escrita e apresenta uma barra de progresso ao longo da execu√ß√£o.

7. Medi√ß√µes de Performance sendo que ao final, mostra o tempo total de execu√ß√£o e o tamanho real do arquivo gerado.

### OUTPUT FILE

Arquivo gerado `data/weather_stations.csv` em 6 min e 5 seg, com 14.8 GiB, somando 1 bilh√£o de linhas.

```text
<nome_da_esta√ß√£o>;<temperatura>
```
- nome_da_esta√ß√£o: string
- temperatura: float com precis√£o de duas casas decimais

Exemplo:

```text
Stockholm;-5.32
S√£o Paulo;25.85
Cape Town;19.01
```

### INTERESTING TECHNICAL POINTS

- Evita uso de `round()` para performance, usando `f"{x:.1f}"` para limitar casas decimais
- Usa `random.choices()` para gerar esta√ß√µes com distribui√ß√£o uniforme entre nomes v√°lidos
- Escreve dados em lote para evitar overhead de I/O, linha a linha
- Estima o uso de disco antes da gera√ß√£o, com fun√ß√£o personalizada para convers√£o de bytes
- Fornece mensagens amig√°veis de erro e ajuda ao usu√°rio


### IMPLEMENTED APPROACHES

üîπ Leitura Linha a Linha (Streaming Puro - Python Nativo)
- Uso de leitura sequencial com open() + readline()
- Agrega√ß√µes realizadas em tempo real com dicion√°rios
- Estrat√©gia eficiente em consumo de mem√≥ria (low RAM footprint)
- Ideal para ambientes com recursos limitados

üîπ Chunking Manual (Python Nativo com Divis√£o em Blocos)
- T√©cnica de leitura em blocos (ex: 1 milh√£o de linhas por vez)
- Reduz picos de mem√≥ria e melhora o controle do processamento
- √ötil para ajustes finos de performance e paraleliza√ß√£o

üîπ Pandas (DataFrame Completo)
- Abordagem de leitura em lote √∫nico com pd.read_csv()
- Permite uso de fun√ß√µes vetorizadas e agrega√ß√µes r√°pidas
- Limita√ß√µes para m√°quinas com <16GB RAM

üîπ Pandas com Chunking (pd.read_csv(..., chunksize=N))
- Divide o dataset em mini-DataFrames processados iterativamente
- Une a performance do Pandas com escalabilidade de pipelines
- √ötil para ambientes em nuvem com controle de mem√≥ria

üîπ DuckDB (Processamento Colunar com SQL Embutido)
- Engine anal√≠tica colunar embutida (sem servidor)
- Altamente otimizada para workloads de leitura pesada
- Permite uso de SQL para agrega√ß√µes diretas no arquivo .csv
- Suporte nativo a .parquet, integra√ß√£o direta com Pandas, Apache Arrow e Python

---

## HOW TO RUN

### REQUIREMENTS

1. Git e Github: Utilizado para versionamento do c√≥digo e para reposit√≥rio remoto do projeto.
Voc√™ deve ter o Git instalado em sua m√°quina e tamb√©m deve ter uma conta no GitHub.
[Instru√ß√µes de instala√ß√£o do Git aqui](https://git-scm.com/doc).
[Instru√ß√µes de instala√ß√£o do Github aqui](https://docs.github.com/pt).

2. Pyenv: √â usado para gerenciar vers√µes do Python em ambientes virtuais, fundamental para isolar a aplica√ß√£o e evitar problemas de conflitos entre vers√µes de bibliotecas e do pr√≥prio Python.
[Instru√ß√µes de instala√ß√£o do Pyenv aqui](https://github.com/pyenv/pyenv#installation).
Neste projeto, vamos utilizar o Python 3.11.4

3. Poetry: Este projeto utiliza Poetry para gerenciamento de depend√™ncias.
[Instru√ß√µes de instala√ß√£o do Poetry aqui](https://python-poetry.org/docs/#installation).

### INSTALA√á√ÉO E CONFIGURA√á√ÉO

A - Execute o comando, passando os argumentos da quantidade de linhas que quer gerar:

```python
python create_measurements.py 1_000_000_000
```
---

B - Entre no diret√≥rio `/data/`e execute o comando, de acordo com a ferramenta e amodelagem de dados desejada:

i) Python
```python
python etl_python.py
```

ii) Python com chuncking
```python
python etl_python_chuncking.py
```

iii) Pandas
```python
python etl_pandas.py
```

iv) Pandas com chuncking
```python
python etl_pandas_chuncking.py
```
---

C - Instale a biblioteca duckDB, utilizando o Poetry, com o comando:
```python
poetry add duckdb
```

v) duckDB
```python
python etl_duckDB.py
```

## OUTPUT EXAMPLES

Todos os resultados finais s√£o exportados nos formatos .csv e .parquet

Isso permite an√°lises posteriores em ferramentas como Power BI, Metabase, Apache Superset ou puro Python.Formato de sa√≠da (ordenado alfabeticamente por nome da esta√ß√£o):

```python
| Esta√ß√£o    | Min    | M√©dia | Max   |
| ---------- | ------ | ----- | ----- |
| Aabenraa   | -99.80 | 3.4   | 99.80 |
| Bariloche  | -57.40 | 8.2   | 87.30 |
| Copenhagen | -45.50 | 11.9  | 94.10 |
```

---

## TECHNOLOGIES USED

### PROJECT SUPPORT

üîπ Poetry para gerenciamento de depend√™ncias

üîπ Pyenv para isolamento de ambientes

üîπ Pre-commit hooks:
- trailing-whitespace
- end-of-file-fixer
- check-yaml
- check-added-large-files
- check-json
- check-merge-conflict
- check-case-conflict

üîπ Pip-audit

üîπ Black

üîπ Ruff


### PROJECT DEVELOPMENT

üîπPython 3.11+

üîπPandas

üîπDuckDB

üîπPolars

---

## BENCHMARKING AND PERFORMANCE

M√©todo	Tempo Estimado	Uso de Mem√≥ria	Coment√°rios
Python Nativo (streaming)	Alto	Muito baixo	Alta compatibilidade com ambientes limitados
Chunking Manual	M√©dio	Controlado	Equil√≠brio entre controle e simplicidade
Pandas Completo	Baixo*	Alto	Muito r√°pido, mas exige boa RAM
Pandas com Chunking	M√©dio-baixo	Controlado	√ìtima rela√ß√£o performance/mem√≥ria
DuckDB SQL	Baixo	Muito baixo	Ideal para pipelines anal√≠ticos colunarizados

---

## MAIN TECHNICAL FEATURES

‚úÖ Modular function design (read, calculate, format, log)

‚úÖ Log file with timestamps and emojis for readability

‚úÖ Automatic folder creation for logs

‚úÖ Fast performance with native Python (no Pandas or NumPy required)

‚úÖ Friendly CLI usage, expandable to larger systems

---

Project carried out with the support of Artificial Intelligence (ChatGPT)

For future improvements: extraction of real data with cleaning and transformation, followed by loading into a Data Warehouse, possibly in a cloud provider, also, an ETL orchestrated with Apache Airflow and best CI/CD practices


## QUESTIONS, SUGGESTIONS OR FEEDBACK

**üöÄ Andr√© Matiello C. Caramanti - [matiello.andre@hotmail.com](mailto:matiello.andre@hotmail.com)**

---

## LICENSE

[MIT License](https://andrematiello.notion.site/mit-license)
