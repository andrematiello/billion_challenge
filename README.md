# ONE BILLION CHALLENGE ‚Äì PYTHON EDITION

## ABOUT THE PROJECT

Uma jornada pr√°tica e realista de engenharia de dados para processar 1 bilh√£o de registros, extraindo estat√≠sticas agregadas de temperatura com performance, escalabilidade em Python, utilizando o projeto One Billion Row Challenge, desenvolvido como um exerc√≠cio avan√ßado de engenharia de dados aplicada, com o objetivo de demonstrar como processar com efici√™ncia um arquivo massivo de 1 bilh√£o de linhas (~14GB) usando Python, cujo foco est√° em realizar opera√ß√µes computacionalmente intensas como agrega√ß√µes (m√≠nimo, m√©dia e m√°ximo) e ordena√ß√£o com uso criterioso de recursos computacionais, de forma escal√°vel.

Este projeto √© particularmente √∫til como estudo de caso para engenheiros de dados, cientistas de dados e desenvolvedores que desejam aprofundar seus conhecimentos em processamento de arquivos massivos, estrat√©gias de chunking, desempenho de bibliotecas Python e uso de engines anal√≠ticas modernas como o DuckDB e embora o One Billion Row Challenge n√£o seja um projeto t√©cnico, ele simula situa√ß√µes reais de neg√≥cio enfrentadas por empresas que lidam com grandes volumes de dados transacionais, sensoriais ou operacionais.

## INSPIRATION

O desafio foi inspirado no projeto original [1BRC](https://github.com/gunnarmorling/1brc), proposto por Gunnar Morling em Java, com o esp√≠rito:

> ‚ÄúExplore at√© onde as linguagens modernas podem ir ao processar um bilh√£o de linhas, use todos os (v)n√∫cleos, SIMD, otimiza√ß√µes de GC... e crie a implementa√ß√£o mais r√°pida para resolver esse problema!‚Äù

Posteriormente, a iniciativa foi adaptada para Python por Luciano Vasconcelos, no reposit√≥rio [One-Billion-Row-Challenge-Python](https://github.com/lvgalvao/One-Billion-Row-Challenge-Python), como um workshop, dentro do contexto educacional da Jornada de Dados, em 2024.


---

## BUSINESS PROBLEM

A seguir, destacam-se os principais problemas que esse case ajuda a resolver:

‚û°Ô∏è 1. Processamento de Grandes Volumes de Dados em Arquivos Brutos, quando empresas frequentemente recebem dados em formatos como .csv, .json ou .parquet contendo milh√µes ou bilh√µes de linhas, especialmente em setores como varejo, energia, climatologia, IoT e telecom, demonstrando como ler, limpar e agregar dados diretamente de arquivos massivos, sem a necessidade imediata de carregar tudo na mem√≥ria ou depender de clusters caros.

‚û°Ô∏è 2. C√°lculo Eficiente de Estat√≠sticas Agregadas, por meio da an√°lise de dados operacionais exige c√°lculos como m√©dia, m√°ximo e m√≠nimo, que parecem simples, mas se tornam desafiadores com grande volume e m√∫ltiplas chaves, o case mostra como aplicar estrat√©gias otimizadas de agrega√ß√£o, inclusive via DuckDB ou Pandas com chunking, simulando o c√°lculo de indicadores operacionais em escala.

‚û°Ô∏è 3. Desempenho e Otimiza√ß√£o de Recursos Computacionais, quando projetos de dados nem sempre rodam em ambientes robustos, muitos times enfrentam limita√ß√µes de RAM, CPU e I/O, especialmente em pipelines locais, servidores intermedi√°rios ou jobs agendados, explorando estrat√©gias de baixo consumo de mem√≥ria, chunking e uso de engines colunares (como DuckDB) que permitem otimizar desempenho mesmo em m√°quinas comuns.

‚û°Ô∏è 4. Valida√ß√£o de Arquiteturas Anal√≠ticas para Batch Processing, no processo de valida√ß√£o, por exemplo, se uma arquitetura (ex: processamento local + exporta√ß√£o .parquet) atende aos SLAs de tempo e custo antes de mover dados para a nuvem, fornecendo um sandbox completo e replic√°vel, permitindo testar pipelines de processamento, benchmarkar formatos de arquivo e comparar abordagens de leitura e agrega√ß√£o.

‚û°Ô∏è 5. Treinamento e Capacita√ß√£o T√©cnica de Times de Dados, para formar times com maturidade em engenharia de dados exige cases pr√°ticos e desafiadores, que v√£o al√©m de notebooks pequenos ou datasets de toy, demonstrando ser um estudo de caso avan√ßado que pode ser usado para treinar engenheiros, analistas e cientistas de dados, com foco em performance, arquitetura de dados e boas pr√°ticas de codifica√ß√£o.

‚û°Ô∏è 6. Exporta√ß√£o de Dados para Consumo em BI e Visualiza√ß√µes, etapa comum a necessidade de transformar arquivos brutos em formatos eficientes para dashboards (como .csv limpo ou .parquet otimizado), gerando outputs padronizados e ordenados para ingest√£o por ferramentas como Power BI, Metabase, Superset ou solu√ß√µes em nuvem, com foco em consumo r√°pido e leve.

---

## PROPOSED CHALLENGE

Desenvolver solu√ß√µes em Python para:

üîπLer o arquivo de entrada com 1 bilh√£o de linhas

üîπCalcular, para cada esta√ß√£o:
- Temperatura m√≠nima
- Temperatura m√°xima
- Temperatura m√©dia (com 2 casas decimais)

üîπOrdenar os resultados por nome da esta√ß√£o

üîπExportar os resultados para os formatos .csv e .parquet

üîπComparar diferentes abordagens de performance, mem√≥ria e escalabilidade

### DATA STRUCTURE

O arquivo de entrada cont√©m medi√ß√µes de temperatura de diferentes esta√ß√µes meteorol√≥gicas, com o seguinte formato por linha:

---

### GENERAL OPERATION

1. Valida√ß√£o dos Argumentos, pois o script recebe como argumento a quantidade de linhas a serem geradas.

2. Coleta de Nomes de Esta√ß√µes, l√™ um arquivo chamado `model.csv` com nomes de esta√ß√µes meteorol√≥gicas.

3. Remove duplicatas e ignora linhas comentadas com `#`, ainda devolve uma estimativa de tamanho do arquivo

4. Calcula o tamanho estimado do arquivo final com base na m√©dia de caracteres dos nomes das esta√ß√µes e nas temperaturas geradas, com a gera√ß√£o de dados sint√©ticos

5. Cria medi√ß√µes com temperaturas aleat√≥rias entre -99.9¬∞C e 99.9¬∞C e gera o arquivo `data/weather_stations.csv`.

6. Utiliza processamento em batches de 10.000 registros para melhor desempenho de escrita e apresenta uma barra de progresso ao longo da execu√ß√£o.

7. Medi√ß√µes de Performance sendo que ao final, mostra o tempo total de execu√ß√£o e o tamanho real do arquivo gerado.

---

### OUTPUT FILE

Arquivo gerado `data/weather_stations.csv` em 6 min e 5 seg, com 14.8 GiB, somando 1 bilh√£o de linhas.

```text
<nome_da_esta√ß√£o>;<temperatura>
```
- nome_da_esta√ß√£o: string
- temperatura: float com precis√£o de duas casas decimais

Exemplo:

```text
Stockholm;-5.32
S√£o Paulo;25.85
Cape Town;19.01
```
---

### INTERESTING TECHNICAL POINTS

- Evita uso de `round()` para performance, usando `f"{x:.1f}"` para limitar casas decimais
- Usa `random.choices()` para gerar esta√ß√µes com distribui√ß√£o uniforme entre nomes v√°lidos
- Escreve dados em lote para evitar overhead de I/O, linha a linha
- Estima o uso de disco antes da gera√ß√£o, com fun√ß√£o personalizada para convers√£o de bytes
- Fornece mensagens amig√°veis de erro e ajuda ao usu√°rio

---

### IMPLEMENTED APPROACHES

üîπ Leitura Linha a Linha (Streaming Puro - Python Nativo)
- Uso de leitura sequencial com open() + readline()
- Agrega√ß√µes realizadas em tempo real com dicion√°rios
- Estrat√©gia eficiente em consumo de mem√≥ria (low RAM footprint)
- Ideal para ambientes com recursos limitados

üîπ Chunking Manual (Python Nativo com Divis√£o em Blocos)
- T√©cnica de leitura em blocos (ex: 1 milh√£o de linhas por vez)
- Reduz picos de mem√≥ria e melhora o controle do processamento
- √ötil para ajustes finos de performance e paraleliza√ß√£o

üîπ Pandas (DataFrame Completo)
- Abordagem de leitura em lote √∫nico com pd.read_csv()
- Permite uso de fun√ß√µes vetorizadas e agrega√ß√µes r√°pidas
- Limita√ß√µes para m√°quinas com <16GB RAM

üîπ Pandas com Chunking (pd.read_csv(..., chunksize=N))
- Divide o dataset em mini-DataFrames processados iterativamente
- Une a performance do Pandas com escalabilidade de pipelines
- √ötil para ambientes em nuvem com controle de mem√≥ria

üîπ DuckDB (Processamento Colunar com SQL Embutido)
- Engine anal√≠tica colunar embutida (sem servidor)
- Altamente otimizada para workloads de leitura pesada
- Permite uso de SQL para agrega√ß√µes diretas no arquivo .csv
- Suporte nativo a .parquet, integra√ß√£o direta com Pandas, Apache Arrow e Python

---

## HOW TO RUN

### REQUIREMENTS

1. Git e Github: Utilizado para versionamento do c√≥digo e para reposit√≥rio remoto do projeto.
Voc√™ deve ter o Git instalado em sua m√°quina e tamb√©m deve ter uma conta no GitHub.
[Instru√ß√µes de instala√ß√£o do Git aqui](https://git-scm.com/doc).
[Instru√ß√µes de instala√ß√£o do Github aqui](https://docs.github.com/pt).

2. Pyenv: √â usado para gerenciar vers√µes do Python em ambientes virtuais, fundamental para isolar a aplica√ß√£o e evitar problemas de conflitos entre vers√µes de bibliotecas e do pr√≥prio Python.
[Instru√ß√µes de instala√ß√£o do Pyenv aqui](https://github.com/pyenv/pyenv#installation).
Neste projeto, vamos utilizar o Python 3.11.4

3. Poetry: Este projeto utiliza Poetry para gerenciamento de depend√™ncias.
[Instru√ß√µes de instala√ß√£o do Poetry aqui](https://python-poetry.org/docs/#installation).

---

### INSTALA√á√ÉO E CONFIGURA√á√ÉO

A - Execute o comando, passando os argumentos da quantidade de linhas que quer gerar:

```python
python create_measurements.py 1_000_000_000
```
---

B - Confirmar a quantidade de linhas e o formato do arquivo gerado:
```python
wc -l ../data/weather_stations.csv
head -n 5 ../data/weather_stations.csv
```

C - Entre no diret√≥rio `/data/`e execute o comando, de acordo com a ferramenta e amodelagem de dados desejada:

i) Python - processamento BRUTO com `defaultdict`, utilizando Python vanilla!
```python
python etl_python.py
```

ii) Python com chuncking
```python
python etl_python_chuncking.py
```

iii) Pandas
```python
python etl_pandas.py
```

iv) Pandas com chuncking
```python
python etl_pandas_chuncking.py
```
---

D - Instale a biblioteca duckDB, utilizando o Poetry, com o comando:
```python
poetry add duckdb
```

v) duckDB
```python
python etl_duckDB.py
```

---

## OUTPUT EXAMPLES

Todos os resultados finais s√£o exportados nos formatos .csv e .parquet

Isso permite an√°lises posteriores em ferramentas como Power BI, Metabase, Apache Superset ou puro Python, inclusive, o arquivo de sa√≠da ser√° ordenado alfabeticamente por nome da esta√ß√£o:

```python
| Esta√ß√£o    | Min    | M√©dia | Max   |
| ---------- | ------ | ----- | ----- |
| Aabenraa   | -99.80 | 3.4   | 99.80 |
| Bariloche  | -57.40 | 8.2   | 87.30 |
| Copenhagen | -45.50 | 11.9  | 94.10 |
```

---

## TECHNOLOGIES USED

### PROJECT SUPPORT

üîπ Poetry para gerenciamento de depend√™ncias

üîπ Pyenv para isolamento de ambientes

üîπ Pre-commit hooks:
- trailing-whitespace
- end-of-file-fixer
- check-yaml
- check-added-large-files
- check-json
- check-merge-conflict
- check-case-conflict

üîπ Pip-audit

üîπ Black

üîπ Ruff

---

### PROJECT DEVELOPMENT

üîπPython 3.11+

üîπPandas

üîπPyarrow

üîπPolars

üîπDuckDB

---

## BENCHMARKING AND PERFORMANCE

### PYTHON
üî¥ Python vanilla, sem utiliza√ß√£o de ulimit ou cgroups.
A ETL quebrou por 6 vezes, consumindo os 16 GiB (15.3) de mem√≥ria RAM do servidor e mais 4 de Swp

üü® Python Vanilla com melhorias de performance:
A ETL rodou satisfatoriamente, demorando 726.20 segundos (pouco mais de 12 minutos) e consumindo apenas 1.5 GiB de mem√≥ria RAM, no momento de pico de utiliza√ß√£o do sistema.

üü® Python com a utiliza√ß√£o de t√©cnica de chunking
A ETL rodou sofrida, n√£o aguentou com chuncking de 100 milh√µes de linhas, quebrando duas vezes, rodando com chuncking de 50 milh√µes de linhas em 20 etapas, demorando 1436.41 segundos (quase 24 minutos) e consumindo 12.2 GiB de mem√≥ria RAM, no momento de pico de utiliza√ß√£o do sistema.

---

### PYTHON + PYARROW
üü® Python com a utiliza√ß√£o da biblioteca pyarrow apenas para gravar o parquet.
A ETL rodou satisfatoriamente, demorando 711.31 segundos (quase 12 minutos) e consumindo apenas 1.2 GiB de mem√≥ria RAM, no momento de pico de utiliza√ß√£o do sistema.

---

### PYTHON + PANDAS
üî¥ Python + Pandas na leitura e no processamento
A ETL quebrou por 3 vezes, consumindo os 16 GiB (15.3) de mem√≥ria RAM do servidor e mais 4 de Swp

üü® Python + Pandas na leitura e no processamento + utiliza√ß√£o de t√©cnica de chunking
A ETL rodou satisfatoriamente, rodou com chuncking de 100 milh√µes de linhas, demorando 348.58 segundos (quase 6 minutos) e consumindo 10 GiB de mem√≥ria RAM, no momento de pico de utiliza√ß√£o do sistema.

---

### PYTHON + POLARS
üî¥ Python + Polars na leitura e no processamento
A ETL quebrou por 3 vezes, em 5 segundos, consumindo os 16 GiB (15.3) de mem√≥ria RAM do servidor e mais 4 de Swp

üî¥ Python + Polars na leitura e no processamento + utiliza√ß√£o de t√©cnica de paralelismo
A ETL quebrou por 3 vezes, em 5 segundos, consumindo os 16 GiB (15.3) de mem√≥ria RAM do servidor e mais 4 de Swp

---

### duckDB
üü¢ Utiliza√ß√£o do banco de dados duckDB ü•á üèÜ
A ETL rodou lisa, demorando 12.38 segundos e consumindo apenas 1.76 GiB de mem√≥ria RAM, no momento de pico de utiliza√ß√£o do sistema.


## CONCLUSION

O benchmark conduzido com 1 bilh√£o de registros sint√©ticos de esta√ß√µes meteorol√≥gicas revela insights importantes sobre tempo de execu√ß√£o, uso de mem√≥ria, tamanho dos arquivos e escalabilidade entre diferentes estrat√©gias de processamento: Python puro, Pandas, abordagens com chunking, Polars e DuckDB.

### 1. ‚è±Ô∏è Tempo de Execu√ß√£o Total

- DuckDB manteve seu desempenho superior, concluindo a ETL em apenas 12.38 segundos, mesmo com 1 bilh√£o de linhas.
- Pandas com chunking foi a abordagem tradicional mais eficiente, concluindo em 348.58 segundos (~6 minutos).
- Python puro com melhorias levou 726.20 segundos (~12 minutos), com performance est√°vel.
- Python com chunking precisou de m√∫ltiplas etapas (20 chunks de 50 milh√µes), totalizando 1436.41 segundos (~24 minutos).
- As abordagens com Polars e Pandas sem chunking falharam devido ao estouro de mem√≥ria, n√£o completando a execu√ß√£o.

![total_time](image.png)

DuckDB novamente se destaca por sua efici√™ncia vetorizada e engine SQL em mem√≥ria. Chunking com Pandas ou Python √© mais lento, mas confi√°vel quando h√° limita√ß√£o de RAM.

---

### 2. üíæ Pico de Uso de Mem√≥ria RAM

- Python + PyArrow (escrevendo apenas Parquet com PyArrow) foi o mais econ√¥mico, com pico de 1.2 GiB.
- DuckDB tamb√©m se manteve enxuto, consumindo apenas 1.76 GiB.
- Python + melhorias estabilizou em 1.5 GiB.
- Pandas com chunking usou 10 GiB, demonstrando bom controle.
- Python com chunking chegou a 12.2 GiB.
- Pandas, Polars e outras abordagens sem chunking estouraram os 16 GiB de RAM + 4 GiB de swap, travando a execu√ß√£o.

![total_memory](image-1.png)

DuckDB e PyArrow mant√™m uso controlado de mem√≥ria. Abordagens com chunking consomem mais, mas s√£o seguras. Estrat√©gias sem chunking falham com volumes bilion√°rios.

---

### 3. üì¶ Tamanho dos Arquivos (MiB)

Todos os arquivos CSV t√™m tamanho semelhante (~252 KB). DuckDB gerou o menor .csv e tamb√©m o .parquet mais compacto, evidenciando compress√£o eficiente e escrita otimizada.

![total_file_size](image-2.png)

### ## Considera√ß√µes de Arquitetura e Escalabilidade

- DuckDB permanece como a op√ß√£o mais r√°pida, leve e escal√°vel para an√°lise local, com excelente performance mesmo com 1 bilh√£o de registros.
- Pandas + chunking se mostra um bom compromisso para ambientes com restri√ß√£o de mem√≥ria, sem comprometer robustez.
- Python puro com chunking √© funcional, mas requer ajustes e monitoramento rigoroso de recursos.
- Polars ainda n√£o sustentou o volume testado ‚Äî falhou em todas as tentativas mesmo com paralelismo ativado.

### ## Recomenda√ß√£o Final

Para pipelines de grande volume com baixa complexidade de transforma√ß√£o e foco em performance:

- ‚úÖ DuckDB continua imbat√≠vel: r√°pido, econ√¥mico e com boa compress√£o.
- üü° Pandas com chunking √© seguro, compat√≠vel e f√°cil de manter.
- üü° Python com chunking √© defens√°vel, mas exige mais trabalho manual.
- üî¥ Abordagens sem chunking n√£o s√£o recomendadas acima de 1 bilh√£o de linhas.

---

## MAIN TECHNICAL FEATURES

‚úÖ Modular function design (read, calculate, format, log)

‚úÖ Log file with timestamps and emojis for readability

‚úÖ Automatic folder creation for logs

‚úÖ Fast performance with native Python (no Pandas or NumPy required)

‚úÖ Friendly CLI usage, expandable to larger systems

---

Project carried out with the support of Artificial Intelligence (ChatGPT)

For future improvements: extraction of real data with cleaning and transformation, followed by loading into a Data Warehouse, possibly in a cloud provider, also, an ETL orchestrated with Apache Airflow and best CI/CD practices


## QUESTIONS, SUGGESTIONS OR FEEDBACK

üöÄ Andr√© Matiello C. Caramanti - [matiello.andre@hotmail.com](mailto:matiello.andre@hotmail.com)

---

## LICENSE

[MIT License](https://andrematiello.notion.site/mit-license)
