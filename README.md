# ONE BILLION CHALLENGE (OBRC) â€“ PYTHON EDITION

## ABOUT THE PROJECT

Uma jornada prÃ¡tica e realista de engenharia de dados para processar 1 bilhÃ£o de registros, extraindo estatÃ­sticas agregadas de temperatura com performance, escalabilidade em Python. O projeto One Billion Row Challenge (OBRC) foi desenvolvido como um exercÃ­cio avanÃ§ado de engenharia de dados aplicada, com o objetivo de demonstrar como processar com eficiÃªncia um arquivo massivo de 1 bilhÃ£o de linhas (~14GB) usando Python, cujo foco estÃ¡ em realizar operaÃ§Ãµes computacionalmente intensas como agregaÃ§Ãµes (mÃ­nimo, mÃ©dia e mÃ¡ximo) e ordenaÃ§Ã£o com uso criterioso de recursos computacionais.

Este projeto Ã© particularmente Ãºtil como estudo de caso para engenheiros de dados, cientistas de dados e desenvolvedores que desejam aprofundar seus conhecimentos em processamento de arquivos massivos, estratÃ©gias de chunking, desempenho de bibliotecas Python e uso de engines analÃ­ticas modernas como o DuckDB e embora o One Billion Row Challenge nÃ£o seja um projeto tÃ©cnico, ele simula situaÃ§Ãµes reais de negÃ³cio enfrentadas por empresas que lidam com grandes volumes de dados transacionais, sensoriais ou operacionais.

## BUSINESS PROBLEM

A seguir, destacam-se os principais problemas que esse case ajuda a resolver:

â¡ï¸ 1. Processamento de Grandes Volumes de Dados em Arquivos Brutos, quando empresas frequentemente recebem dados em formatos como .csv, .json ou .parquet contendo milhÃµes ou bilhÃµes de linhas, especialmente em setores como varejo, energia, climatologia, IoT e telecom, demonstrando como ler, limpar e agregar dados diretamente de arquivos massivos, sem a necessidade imediata de carregar tudo na memÃ³ria ou depender de clusters caros.

â¡ï¸ 2. CÃ¡lculo Eficiente de EstatÃ­sticas Agregadas, por meio da anÃ¡lise de dados operacionais exige cÃ¡lculos como mÃ©dia, mÃ¡ximo e mÃ­nimo, que parecem simples, mas se tornam desafiadores com grande volume e mÃºltiplas chaves, o case mostra como aplicar estratÃ©gias otimizadas de agregaÃ§Ã£o, inclusive via DuckDB ou Pandas com chunking, simulando o cÃ¡lculo de indicadores operacionais em escala.

â¡ï¸ 3. Desempenho e OtimizaÃ§Ã£o de Recursos Computacionais, quando projetos de dados nem sempre rodam em ambientes robustos, muitos times enfrentam limitaÃ§Ãµes de RAM, CPU e I/O, especialmente em pipelines locais, servidores intermediÃ¡rios ou jobs agendados, explorando estratÃ©gias de baixo consumo de memÃ³ria, chunking e uso de engines colunares (como DuckDB) que permitem otimizar desempenho mesmo em mÃ¡quinas comuns.

â¡ï¸ 4. ValidaÃ§Ã£o de Arquiteturas AnalÃ­ticas para Batch Processing, no processo de validaÃ§Ã£o, por exemplo, se uma arquitetura (ex: processamento local + exportaÃ§Ã£o .parquet) atende aos SLAs de tempo e custo antes de mover dados para a nuvem, fornecendo um sandbox completo e replicÃ¡vel, permitindo testar pipelines de processamento, benchmarkar formatos de arquivo e comparar abordagens de leitura e agregaÃ§Ã£o.

â¡ï¸ 5. Treinamento e CapacitaÃ§Ã£o TÃ©cnica de Times de Dados, para formar times com maturidade em engenharia de dados exige cases prÃ¡ticos e desafiadores, que vÃ£o alÃ©m de notebooks pequenos ou datasets de toy, demonstrando ser um estudo de caso avanÃ§ado que pode ser usado para treinar engenheiros, analistas e cientistas de dados, com foco em performance, arquitetura de dados e boas prÃ¡ticas de codificaÃ§Ã£o.

â¡ï¸ 6. ExportaÃ§Ã£o de Dados para Consumo em BI e VisualizaÃ§Ãµes, etapa comum a necessidade de transformar arquivos brutos em formatos eficientes para dashboards (como .csv limpo ou .parquet otimizado), gerando outputs padronizados e ordenados para ingestÃ£o por ferramentas como Power BI, Metabase, Superset ou soluÃ§Ãµes em nuvem, com foco em consumo rÃ¡pido e leve.

---

## INSPIRATION

O desafio foi inspirado no projeto original [1BRC](https://github.com/gunnarmorling/1brc), proposto por Gunnar Morling em Java, com o espÃ­rito:

> â€œExplore atÃ© onde as linguagens modernas podem ir ao processar um bilhÃ£o de linhas, use todos os (v)nÃºcleos, SIMD, otimizaÃ§Ãµes de GC... e crie a implementaÃ§Ã£o mais rÃ¡pida para resolver esse problema!â€

Posteriormente, a iniciativa foi adaptada para Python por Luciano Vasconcelos, no repositÃ³rio [One-Billion-Row-Challenge-Python](https://github.com/lvgalvao/One-Billion-Row-Challenge-Python), como um workshop, dentro do contexto educacional da Jornada de Dados, em 2024.

---

## DATA STRUCTURE

O arquivo de entrada contÃ©m mediÃ§Ãµes de temperatura de diferentes estaÃ§Ãµes meteorolÃ³gicas, com o seguinte formato por linha:

```text
<nome_da_estaÃ§Ã£o>;<temperatura>
```

nome_da_estaÃ§Ã£o: string
temperatura: float com precisÃ£o de duas casas decimais
Exemplo:
```text
Stockholm;-5.32
SÃ£o Paulo;25.85
Cape Town;19.01
```

---

## PROPOSED CHALLENGE

Desenvolver soluÃ§Ãµes em Python para:

ğŸ”¹Ler o arquivo de entrada com 1 bilhÃ£o de linhas

ğŸ”¹Calcular, para cada estaÃ§Ã£o:
- Temperatura mÃ­nima
- Temperatura mÃ¡xima
- Temperatura mÃ©dia (com 2 casas decimais)

ğŸ”¹Ordenar os resultados por nome da estaÃ§Ã£o

ğŸ”¹Exportar os resultados para os formatos .csv e .parquet

ğŸ”¹Comparar diferentes abordagens de performance, memÃ³ria e escalabilidade

---

## IMPLEMENTED APPROACHES

ğŸ”¹ Leitura Linha a Linha (Streaming Puro - Python Nativo)
- Uso de leitura sequencial com open() + readline()
- AgregaÃ§Ãµes realizadas em tempo real com dicionÃ¡rios
- EstratÃ©gia eficiente em consumo de memÃ³ria (low RAM footprint)
- Ideal para ambientes com recursos limitados

ğŸ”¹ Chunking Manual (Python Nativo com DivisÃ£o em Blocos)
- TÃ©cnica de leitura em blocos (ex: 1 milhÃ£o de linhas por vez)
- Reduz picos de memÃ³ria e melhora o controle do processamento
- Ãštil para ajustes finos de performance e paralelizaÃ§Ã£o

ğŸ”¹ Pandas (DataFrame Completo)
- Abordagem de leitura em lote Ãºnico com pd.read_csv()
- Permite uso de funÃ§Ãµes vetorizadas e agregaÃ§Ãµes rÃ¡pidas
- LimitaÃ§Ãµes para mÃ¡quinas com <16GB RAM

ğŸ”¹ Pandas com Chunking (pd.read_csv(..., chunksize=N))
- Divide o dataset em mini-DataFrames processados iterativamente
- Une a performance do Pandas com escalabilidade de pipelines
- Ãštil para ambientes em nuvem com controle de memÃ³ria

ğŸ”¹ DuckDB (Processamento Colunar com SQL Embutido)
- Engine analÃ­tica colunar embutida (sem servidor)
- Altamente otimizada para workloads de leitura pesada
- Permite uso de SQL para agregaÃ§Ãµes diretas no arquivo .csv
- Suporte nativo a .parquet, integraÃ§Ã£o direta com Pandas, Apache Arrow e Python

---

## OUTPUT EXAMPLES

Todos os resultados finais sÃ£o exportados nos formatos .csv e .parquet

Isso permite anÃ¡lises posteriores em ferramentas como Power BI, Metabase, Apache Superset ou puro Python.Formato de saÃ­da (ordenado alfabeticamente por nome da estaÃ§Ã£o):

```python
| EstaÃ§Ã£o    | Min    | MÃ©dia | Max   |
| ---------- | ------ | ----- | ----- |
| Aabenraa   | -99.80 | 3.4   | 99.80 |
| Bariloche  | -57.40 | 8.2   | 87.30 |
| Copenhagen | -45.50 | 11.9  | 94.10 |
```

---

## TECHNOLOGIES USED

### PROJECT SUPPORT

ğŸ”¹ Poetry para gerenciamento de dependÃªncias

ğŸ”¹ Pyenv para isolamento de ambientes

ğŸ”¹ Pre-commit hooks:
- trailing-whitespace
- end-of-file-fixer
- check-yaml
- check-added-large-files
- check-json
- check-merge-conflict
- check-case-conflict

ğŸ”¹ Pip-audit

ğŸ”¹ Black

ğŸ”¹ Ruff


### PROJECT DEVELOPMENT

ğŸ”¹Python 3.11+

ğŸ”¹Pandas

ğŸ”¹DuckDB

ğŸ”¹Polars

---

## BENCHMARKING AND PERFORMANCE

MÃ©todo	Tempo Estimado	Uso de MemÃ³ria	ComentÃ¡rios
Python Nativo (streaming)	Alto	Muito baixo	Alta compatibilidade com ambientes limitados
Chunking Manual	MÃ©dio	Controlado	EquilÃ­brio entre controle e simplicidade
Pandas Completo	Baixo*	Alto	Muito rÃ¡pido, mas exige boa RAM
Pandas com Chunking	MÃ©dio-baixo	Controlado	Ã“tima relaÃ§Ã£o performance/memÃ³ria
DuckDB SQL	Baixo	Muito baixo	Ideal para pipelines analÃ­ticos colunarizados

---

## MAIN TECHNICAL FEATURES

âœ… Modular function design (read, calculate, format, log)
âœ… Log file with timestamps and emojis for readability
âœ… Automatic folder creation for logs
âœ… Fast performance with native Python (no Pandas or NumPy required)
âœ… Friendly CLI usage, expandable to larger systems

---

Project carried out with the support of Artificial Intelligence (ChatGPT)

For future improvements: extraction of real data with cleaning and transformation, followed by loading into a Data Warehouse, possibly in a cloud provider, also, an ETL orchestrated with Apache Airflow and best CI/CD practices


## QUESTIONS, SUGGESTIONS OR FEEDBACK

**ğŸš€ AndrÃ© Matiello C. Caramanti - [matiello.andre@hotmail.com](mailto:matiello.andre@hotmail.com)**

---

## LICENSE

[MIT License](https://andrematiello.notion.site/mit-license)
